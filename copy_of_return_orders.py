# -*- coding: utf-8 -*-
"""Copy of return_orders.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UlP1U9QtXng8oX6GwxvmZwIP8CwITdii
"""

#Importing Basic Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files

file=files.upload()

df=pd.read_csv('TrainingData_V1.csv')

df.head(5)

df.describe()

df['item_color'].value_counts()

df.shape

"""#Exploratory Data Analysis"""

df.groupby('return')['item_price'].plot(kind='hist')

colors_counts=df['item_id'].value_counts()
color_counts_dict=df['item_color'].value_counts().nlargest(10)
color_counts=pd.DataFrame(color_counts_dict)
color_counts=color_counts.reset_index()

plt.figure(figsize=(10,7))
plt.pie(color_counts['item_color'],labels=color_counts['index'].tolist(),shadow=True,autopct='%1.1f%%')
plt.show()
#It would have been cool to display the colors and their names consistently, but color names like petrol are not defined in Matplotlib's colors

plt.figure(figsize=(10,7))
plt.pie(df.user_title.value_counts(),labels=df.user_title.value_counts().index,autopct='%1.1f%%',labeldistance=1.1)
plt.show()

df.user_title.value_counts().index
bar=dict((df[df['return']==1]['user_title'].value_counts()/df['user_title'].value_counts())*100)
return_number=df[df['return']==1]['user_title'].value_counts().values
plt.figure(figsize=(10,10))
x_axis=np.arange(len(df.user_title.value_counts().index))
plt.bar(x=x_axis-0.2,height=return_number,width=0.4)
plt.bar(x=x_axis+0.2,height=df['user_title'].value_counts().values,width=0.4)

plt.figure(figsize=(12,12))
plt.pie(df['brand_id'].value_counts().nlargest(30),labels=df['brand_id'].value_counts().nlargest(30).keys(),autopct='%1.1f%%')
plt.show()

"""#Feature Engineering"""

df['user_reg_date']

df['user_reg_date']=pd.to_datetime(df['user_reg_date'])

df['user_reg_date'].dt.year

df['user_reg_date'].dt.month

df['user_reg_date'].dt.day

sns.heatmap(df.isna())#Easy way to find out the nulls in a Dataset

df_copy=df.copy()

df_copy['delivery_date'].isna()

order_date1=pd.to_datetime(df_copy['order_date'],format='%d-%m-%Y')

delivery_date1=pd.to_datetime(df_copy['delivery_date'],format='%d-%m-%Y')

delivery_gap=delivery_date1-order_date1

df_copy['delivery_gap']=delivery_gap

df_copy[df_copy['delivery_gap']==df_copy['delivery_gap'].min()]

#Delivery Days in Negative...There is something Wrong Here...Let's Check

drop_negative_date=df_copy[df_copy['delivery_date']<df_copy['order_date']]

df_copy.drop(drop_negative_rows,axis=0,inplace=True)

df_copy['delivery_date_temp']=pd.to_datetime(df_copy['delivery_date'],format='%d-%m-%Y')
df_copy['order_date_temp']=pd.to_datetime(df_copy['order_date'],format='%d-%m-%Y')

drop_negative_rows=df_copy[df_copy['delivery_date_temp']<df_copy['order_date_temp']].index

df_copy.drop(drop_negative_rows,axis=0,inplace=True)

df_copy.delivery_gap.mean()

"""We have found the average Delivery Time as 7 Days.
This is after we have dropped the Problematic Date Fields
** We can use the average days as a mean to fill in the Nulls
"""

df_copy.delivery_gap.value_counts().nlargest(20)

sns.heatmap(df.corr())

df.item_id.value_counts()

#Function to check and return Id columns with distinct Ids
def check_unique_ids(dataframe,cols):
    column_list=cols
    unique_length_dict={}
    df=dataframe
    
    for i in column_list:
        unique_length=len(df[i].value_counts().unique())
        unique_length_dict[i]=unique_length
        if unique_length==1:
            return list(unique_length_dict.keys())
        
    #return unique_length_dict

colums_with_ids=[i for i in df.columns if 'id' in i ]
#check_unique_ids(['order_item_id','item_id')

colums_with_ids



#Function to check and convert DateType Columns
def check_for_date_cols(dataframe):
    df=dataframe
    columns=df.columns
    cols_with_dates=[i for i in df.columns if 'date' in i]
    _cols_with_dates=[i for i in df.columns if 'dob' in i]
    for _cols in _cols_with_dates:
        cols_with_dates.append(str(_cols))
    
    
    for x in cols_with_dates:
        df[x]=pd.to_datetime(df[x],format='%d-%m-%Y')
    return df

check_for_date_cols(df)

df.info()

for i in df.columns:
    print(i ,len(df[i].value_counts().values))

return_freq=dict(df[df['return']==1]['user_id'].value_counts())
#Return Frequency of each user

df[df['return']==0]['user_id'].value_counts()

total_users=dict(df['user_id'].value_counts())

return_user_rate={}
for i in return_freq:
    return_user_rate[i]=return_freq[i]/total_users[i]

return_user_rate

"""#Modeling"""

df_modeling_1=df.copy()

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler

ct=make_column_transformer(
    (MinMaxScaler(),['item_price','brand_id','user_state']),
    (OneHotEncoder(handle_unknown='ignore'),['user_title','item_color','item_size'])
)

df_modeling_1_X=df_modeling_1.drop(['return'],axis=1)
df_modeling_1_y=df_modeling_1['return']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(df_modeling_1_X,df_modeling_1_y,test_size=0.2)

from sklearn.linear_model import LogisticRegression

LogRegresssor=LogisticRegression()

LogRegresssor.fit(X_train,y_train)

ct.fit(X_train)

X_train_transformed=ct.transform(X_train)

X_test_transformed=ct.transform(X_test)

X_train_transformed.toarray()[5]

#Look at the above....sucha a sparse Matrix....But we just started and will impove along the way

LogRegresssor.fit(X_train_transformed,y_train)

#To Resolve the above issue, try using max_iters=1000

LogRegresssor=LogisticRegression(max_iter=10000)

LogRegresssor.fit(X_train_transformed,y_train)

y_preds=LogRegresssor.predict(X_test_transformed)

from sklearn.metrics import accuracy_score, confusion_matrix

cm=confusion_matrix(y_test,y_preds)

cm

accurac_score=accuracy_score(y_test,y_preds)

accurac_score

#Okay just 56% Accuracy,.....not good at all....let's try sometiing else

from sklearn.metrics import classification_report

print(classification_report(y_test,y_preds))

X_train,X_test,y_train,y_test=train_test_split(df_modeling_1_X,df_modeling_1_y,test_size=0.2,random_state=42)

df_modeling_1_X
#Bringing the Date Columns Back

ct.fit(X_train)

X_train_transformed=ct.transform(X_train)
X_test_transformed=ct.transform(X_test)

LogRegresssor.fit(X_train_transformed,y_train)

y_preds_logr=LogRegresssor.predict(X_test_transformed)

cm=confusion_matrix(y_test,y_preds)

cm

print(classification_report(y_test,y_preds))

#Oh No....Gotten Worse....Let's try something Else

from sklearn.tree import DecisionTreeClassifier

Decision_tree_classifier=DecisionTreeClassifier()

Decision_tree_classifier.fit(X_train_transformed,y_train)

y_preds=Decision_tree_classifier.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

from sklearn.ensemble import RandomForestClassifier

randfrstclassifier=RandomForestClassifier()

randfrstclassifier.fit(X_train_transformed,y_train)

y_pred=randfrstclassifier.predict(X_test_transformed)

print(accuracy_score(y_test,y_pred))

#58%...Ok Getting better

from xgboost import XGBClassifier

xgb=XGBClassifier(n_estimators=1000)
xgb.fit(X_train_transformed,y_train)

y_preds=xgb.predict(X_test_transformed)

print(accuracy_score(y_test,y_preds))

"""62%.....That's a lot better than before"""

X_train



df_modeling_2=df.copy()

#As we noticed, the avg. days fo delivery were 7 days....Let's use it to fill in inconsistent data and for training data

df_modeling_2['delivery_days']=0

for i in range(len(df_modeling_2)):
    if df_modeling_2['delivery_date'][i]<df['order_date'][i]:
        df_modeling_2['delivery_days'][i]=7
    else:
        df_modeling_2['delivery_days'][i]=(df_modeling_2['delivery_date'][i]-df['order_date'][i]).days

df_modeling_2

df_modeling_2.delivery_days.isna().sum()

df_modeling_2.delivery_days=df_modeling_2.delivery_days.fillna(7.0)

df_modeling_2

df_modeling_2.user_dob.ffill(inplace=True)

df_modeling_2

sns.heatmap(df_modeling_2.isna())

#We will extract the user's age from user_dob...We are not going to with them Happy Bithdays
import datetime

df_modeling_2['user_age']=datetime.date.today().year-df_modeling_2['user_dob'].dt.year

df_modeling_2

df_modeling_3=df_modeling_2.copy()#Creating a checkpoint for the Data

df_modeling_3.drop(['delivery_date','order_date','user_dob'],axis=1,inplace=True)

df_modeling_3_X=df_modeling_3.drop(['return'],axis=1)
df_modeling_3_y=df_modeling_3['return']

ct=make_column_transformer(
    (OneHotEncoder(),['user_title','item_color','item_size','user_state']),
    (StandardScaler(),['item_price','item_id','brand_id','delivery_days','user_age'])
    )

X_train,X_test,y_train,y_test=train_test_split(df_modeling_3_X,df_modeling_3_y,test_size=0.2,random_state=42)

df_modeling_3_X

ct.fit(df_modeling_3_X)

X_train_transformed=ct.transform(X_train)
X_test_transformed=ct.transform(X_test)

Logclassifier=LogisticRegression(max_iter=10000)
Logclassifier.fit(X_train_transformed,y_train)

y_preds=Logclassifier.predict(X_test_transformed)

cm=confusion_matrix(y_test,y_preds)

cm

print(classification_report(y_test,y_preds))

sns.heatmap(df_modeling_3.corr())

Decision_tree_classifier=DecisionTreeClassifier()

Decision_tree_classifier.fit(X_train_transformed,y_train)

y_preds=Logclassifier.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

randfrstclassifier=RandomForestClassifier()
randfrstclassifier.fit(X_train_transformed,y_train)

y_pred=randfrstclassifier.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

xgb=XGBClassifier(n_estimators=1000)
xgb.fit(X_train_transformed,y_train)

y_preds=xgb.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

df_modeling_3['kfold']=-1

df_modeling_3.sample(frac=1).reset_index(drop=True)

from sklearn.model_selection import KFold

kf=KFold(n_splits=5)

for fold,(trn_,val_) in enumerate(kf.split(X=df_modeling_3)):
    df_modeling_3.loc[val_,'kfold']=fold

df_modeling_3

df_modeling_3.to_csv('training_kfolded.csv')

df_modeling_3['user_reg_year']=df_modeling_3['user_reg_date'].dt.year

data=df_modeling_3[df_modeling_3['kfold']!=4]

X_train=data.drop(['return','kfold'],axis=1)

test=df_modeling_3[df_modeling_3['kfold']==4]

X_test=test.drop(['return','kfold'],axis=1)

y_train=data['return']
y_test=test['return']

ct.fit(X_train)

X_train_transformed=ct.transform(X_train)
X_test_transformed=ct.transform(X_test)





from sklearn.model_selection import RandomizedSearchCV

#Let's try Hyperparameter Optimization
params={
    'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],
    'max_depth': [3,4,5,6,8,10,12,15],
    'min_child_weight': [1,3,5,7],
    'gamma': [0.0,0.1,0.2,0.3,0.4],
    'colsample_bytree':[0.3,0.4,0.5,0.7]
}

randcv=RandomizedSearchCV(xgb,param_distributions=params,n_iter=5,scoring='accuracy',n_jobs=-1,verbose=1)

randcv.fit(X_train_transformed,y_train)

y_preds=randcv.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

randcv.best_estimator_

randcv.best_params_

xgb=XGBClassifier(learning_rate=0.15,colsample_bytree=0.7,max_depth=3,min_child_weight=7,gamma=0.4)

xgb.fit(X_train_transformed,y_train)

y_pred=xgb.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

"""It's still 64%"""

#Let's try using MinMaxScalar instead of Standard Scalar

ct=make_column_transformer(
    (OneHotEncoder(),['user_title','user_state','user_reg_year']),
    (MinMaxScaler(),['item_price','item_id','brand_id','delivery_days','user_age'])
    )

ct.fit(X_train)
X_train_transformed=ct.transform(X_train)

X_test_transformed=ct.transform(X_test)

randfrstclassifier=RandomForestClassifier()
randfrstclassifier.fit(X_train_transformed,y_train)

y_preds=randfrstclassifier.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

from sklearn.svm import SVC

svclassifier=SVC()

svclassifier.fit(X_train_transformed,y_train)

y_preds=svclassifier.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

#Applying Deep Learining
import tensorflow as tf

X_test_transformed.shape

tf.random.set_seed(42)

model_9=tf.keras.Sequential([tf.keras.layers.Dense(28, activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)])

model_9.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.Adam(learning_rate=0.02),
                metrics=['accuracy'])

history=model_9.fit(X_train_transformed.toarray(),y_train,epochs=50)

model_9.evaluate(X_test_transformed.toarray(),y_test)

"""Not Much Efffective....Let's try something else"""

from sklearn.naive_bayes import MultinomialNB

MNB=MultinomialNB()

MNB.fit(X_train_transformed,y_train)

y_preds=MNB.predict(X_test_transformed)

print(classification_report(y_test,y_preds))

"""No...Gotten Even Worse...looks like RandomForest and DecissionTree are working best with the Data.....These are tree based Models and does not require us to make any transformations to the Data"""

from sklearn.naive_bayes import GaussianNB

GNB=GaussianNB()
GNB.fit(X_train_transformed.toarray(),y_train)

y_preds=GNB.predict(X_test_transformed.toarray())

print(classification_report(y_test,y_preds))

"""Nope...It's gottenn the same results"""

tf.random.set_seed(42)

model_8=tf.keras.Sequential([tf.keras.layers.Dense(4, activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(4,activation=tf.keras.activations.relu),
                             tf.keras.layers.Dense(1,activation=tf.keras.activations.sigmoid)])

model_8.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

#Create a learning rate callback
lr_scheduler= tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))#Every epoch to start with 1/10exp(-4) and increase by 10 exp(epoch/20)

#Fit the model(pass the lr_scheduler callback)
history=model_8.fit(X_train_transformed.toarray(),y_train,epochs=50,callbacks=[lr_scheduler])

"""Doesn't look promessing....Let's try to work with the Data further
We will try to reduce th number of categories in the Dataset of item_color and Item_size
"""

df_modeling_3['item_color'].value_counts().nlargest(10).sum()/len(df_modeling_3)#Top 10 covers 73% of the data....it makes sense to mark the rest as others.

df_modeling_3.item_size.value_counts().nlargest(10).sum()/len(df_modeling_3)#Top 10 covers 77% of the data....it makes sense to mark the rest as others.

color_accepted=list(df_modeling_3['item_color'].value_counts().nlargest(10).keys())

color_list=[]
for i in df_modeling_3['item_color']:
  if i not in color_accepted:
    color_list.append('others')
  else:
    color_list.append(i)

df_modeling_3_test=df_modeling_3.copy()

df_modeling_3_test['item_color']=color_list

size_accepted=list(df_modeling_3['item_size'].value_counts().nlargest(10).keys())

size_list=[]
for i in df_modeling_3['item_size']:
  if i not in size_accepted:
    size_list.append('others')
  else:
    size_list.append(i)

df_modeling_3_test['item_size']=size_list

df_modeling_3_test

from sklearn.preprocessing import LabelEncoder

LE=LabelEncoder()

df_modeling_3_test['item_color']=LE.fit_transform(df_modeling_3_test['item_color'])

df_modeling_3_test['item_size']=LE.fit_transform(df_modeling_3_test['item_size'])

df_modeling_3_test.corr()

df_modeling_3_test.groupby(['brand_id','item_color','item_size']).count()

df_modeling_3_test['user_title']=LE.fit_transform(df_modeling_3_test['user_title'])

df_modeling_3_test['user_reg_year']=LE.fit_transform(df_modeling_3_test['user_reg_year'])

df_modeling_5_test=df_modeling_3_test.drop(['user_reg_date','order_item_id','user_state','user_id'],axis=1)

X_train,X_test=df_modeling_5_test[df_modeling_5_test['kfold']!=4],df_modeling_5_test[df_modeling_5_test['kfold']==4]

y_train,y_test=X_train['return'],X_test['return']

X_train,X_test=X_train.drop(['return','kfold'],1),X_test.drop(['return','kfold'],1)

xgb=XGBClassifier(learning_rate=0.15,colsample_bytree=0.7,max_depth=3,min_child_weight=7,gamma=0.4)
xgb.fit(X_train,y_train)

y_preds=xgb.predict(X_test)

print(classification_report(y_test,y_preds))

randfrstclassifier=RandomForestClassifier(n_estimators=300,n_jobs=-1)
randfrstclassifier.fit(X_train,y_train)

y_preds=randfrstclassifier.predict(X_test)

print(classification_report(y_test,y_preds))

#Check for price outliers

df_modeling_5_test.describe()

df_modeling_5_test.drop(df_modeling_5_test[df_modeling_5_test['item_price']>225].index,axis=0,inplace=True)

df_modeling_5_test.drop(df_modeling_5_test[df_modeling_5_test['item_price']==0].index,axis=0,inplace=True)

df_modeling_5_test.drop(['kfold'],1,inplace=True)

df_modeling_5_test_X,df_modeling_5_test_y=df_modeling_5_test.drop(['return'],1),df_modeling_5_test['return']

X_train,X_test,y_train,y_test=train_test_split(df_modeling_5_test_X,df_modeling_5_test_y,test_size=0.2)



X_train.user_age.mean()

randfrstclassifier=RandomForestClassifier(n_estimators=300,n_jobs=-1)
randfrstclassifier.fit(X_train,y_train)

y_preds=randfrstclassifier.predict(X_test)

print(classification_report(y_test,y_preds))

importances = randfrstclassifier.feature_importances_
indices = np.argsort(importances)
features = X_train.columns

plt.figure(figsize=(10,15))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

#Let's try to drop user_age

X_train_user_age,X_test_user_age=X_train.drop(['user_age'],1),X_test.drop(['user_age'],1)

randfrstclassifier=RandomForestClassifier(n_estimators=300,n_jobs=-1)
randfrstclassifier.fit(X_train_user_age,y_train)

y_preds=randfrstclassifier.predict(X_test_user_age)

print(classification_report(y_test,y_preds))

df_modeling_5_test

xgb=XGBClassifier(learning_rate=0.15,colsample_bytree=0.7,max_depth=3,min_child_weight=7,gamma=0.4)
xgb.fit(X_train_user_age,y_train)

from sklearn.neighbors import KNeighborsClassifier
KNN = KNeighborsClassifier(n_neighbors = 7)

KNN.fit(X_train_user_age,y_train)

y_preds=KNN.predict(X_test_user_age)

print(classification_report(y_test,y_preds))

"""#Well We are getting at best 64% Accuracy with RandomForest Classifier till now....We will use it for Predictions on the Test Data"""

files.upload()

test_data=pd.read_excel('TestingData_For_Candidate.xlsx')

test_data

"""Converting and Engineering the Test data as per the training Data.....It would have been a good idea to combine the Data first and then do the conversion on both together...but for next attempt"""

test_data.drop(['order_item_id'],1,inplace=True)

check_for_date_cols(test_data)

test_data['delivery_days']=0

for i in range(len(test_data)):
    if test_data['delivery_date'][i]<test_data['order_date'][i]:
        test_data['delivery_days'][i]=7
    else:
        test_data['delivery_days'][i]=(test_data['delivery_date'][i]-df['order_date'][i]).days



test_data.delivery_days=test_data.delivery_days.fillna(7.0)

test_data

test_data.user_dob.ffill()

test_data['user_age']=datetime.date.today().year-test_data['user_dob'].dt.year

test_data

test_data.drop(['order_date','delivery_date','user_dob'],1,inplace=True)

test_data

color_list=[]
for i in test_data['item_color']:
  if i not in color_accepted:
    color_list.append('others')
  else:
    color_list.append(i)

color_list

test_data['item_color']=color_list

test_data

size_list=[]
for i in test_data['item_size']:
  if i not in size_accepted:
    size_list.append('others')
  else:
    size_list.append(i)

test_data['item_size']=size_list

test_data

test_data['user_reg_year']=test_data['user_reg_date'].dt.year

test_data

test_data.drop(['user_reg_date'],1,inplace=True)

test_data.drop(['user_id','user_state'],1,inplace=True)

test_data.item_color.value_counts()

test_data['item_color']=LE.fit_transform(test_data['item_color'])

test_data['item_size']=LE.fit_transform(test_data['item_size'])

test_data['user_title']=LE.fit_transform(test_data['user_title'])

test_data

test_data['user_reg_year']=LE.fit_transform(test_data['user_reg_year'])

test_data

y_results=randfrstclassifier.predict(test_data)

test_data.user_age.fillna(57,inplace=True)#Filling the users age with mean of Training Data

y_results=randfrstclassifier.predict(test_data)

y_results=pd.DataFrame(y_results)



y_results.to_csv('Test_Results.csv')

y_results.shape

